{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pyspark\nfrom pyspark.sql import SparkSession\n\nuser = username\npasswd = passwd\ns3_bucket = bucket\ns3_server = s3_server_url\ns3_access_key = accesskey\ns3_secret_key = secretkey\nmongo_uri = f\"mongodb://{user}:{passwd}@mongo:27017/admin?authSource=admin\"\nserver_name = \"jdbc:sqlserver://mssql\"\ndatabase_name = dbname\nmssql_user = sql_user\nmssql_pw = sql_passwd\nmssql_url = server_name + \";\" + \"databaseName=\" + database_name + \";encrypt=true;trustServerCertificate=true;\"\n\njars = [\n    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n    \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\",\n    \"com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre11\"\n]\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName('jupyter-pyspark') \\\n        .config(\"spark.jars.packages\",\",\".join(jars) )\\\n        .config(\"spark.hadoop.fs.s3a.endpoint\",s3_server_url) \\\n        .config(\"spark.hadoop.fs.s3a.access.key\", accesskey) \\\n        .config(\"spark.hadoop.fs.s3a.secret.key\", secretkey) \\\n        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n    .getOrCreate()\nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")",
      "metadata": {},
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "WARNING: An illegal reflective access operation has occurred\n\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n\nWARNING: All illegal access operations will be denied in a future release\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n\nThe jars for the packages stored in: /home/jovyan/.ivy2/jars\n\norg.apache.hadoop#hadoop-aws added as a dependency\n\norg.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n\ncom.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n\ncom.microsoft.sqlserver#mssql-jdbc added as a dependency\n\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-f5ae1c7d-162c-4b8d-b661-f36ad254e0b3;1.0\n\n\tconfs: [default]\n\n\tfound org.apache.hadoop#hadoop-aws;3.1.2 in central\n\n\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n\n\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n\n\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n\n\tfound org.mongodb#bson;4.0.5 in central\n\n\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n\n\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.2.0 in central\n\n\tfound com.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11 in central\n\n:: resolution report :: resolve 1607ms :: artifacts dl 87ms\n\n\t:: modules in use:\n\n\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n\n\tcom.microsoft.azure#spark-mssql-connector_2.12;1.2.0 from central in [default]\n\n\tcom.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11 from central in [default]\n\n\torg.apache.hadoop#hadoop-aws;3.1.2 from central in [default]\n\n\torg.mongodb#bson;4.0.5 from central in [default]\n\n\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n\n\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n\n\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n\n\t---------------------------------------------------------------------\n\n\t|                  |            modules            ||   artifacts   |\n\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\n\t---------------------------------------------------------------------\n\n\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n\n\t---------------------------------------------------------------------\n\n:: retrieving :: org.apache.spark#spark-submit-parent-f5ae1c7d-162c-4b8d-b661-f36ad254e0b3\n\n\tconfs: [default]\n\n\t0 artifacts copied, 8 already retrieved (0kB/47ms)\n\n23/03/12 18:47:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n\nSetting default log level to \"WARN\".\n\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
        }
      ],
      "id": "2a9ad58a-dd5b-4fb5-b31f-253fdd8aa690"
    },
    {
      "cell_type": "code",
      "source": "from pyspark.sql.functions import split, col, expr, sum, row_number, struct, lit, when, collect_list\nfrom pyspark.sql.column import Column\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType, LongType,ArrayType",
      "metadata": {},
      "execution_count": 2,
      "outputs": [],
      "id": "71a843ea-c445-4510-b89e-64ddf173e17c"
    },
    {
      "cell_type": "code",
      "source": "players = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .option(\"url\", mssql_url) \\\n    .option(\"dbtable\", tablename) \\\n    .option(\"user\", mssql_user) \\\n    .option(\"password\", mssql_pw) \\\n    .load()",
      "metadata": {},
      "execution_count": 3,
      "outputs": [],
      "id": "68c0ae8c-0e0f-41df-9d2c-e814ff0ac077"
    },
    {
      "cell_type": "code",
      "source": "teams = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .option(\"url\", mssql_url) \\\n    .option(\"dbtable\", tablename2) \\\n    .option(\"user\", mssql_user) \\\n    .option(\"password\", mssql_pw) \\\n    .load()",
      "metadata": {},
      "execution_count": 4,
      "outputs": [],
      "id": "fd2632e6-0227-4ca2-a2ae-9db3fa0287ef"
    },
    {
      "cell_type": "code",
      "source": "## Q4\ngamestream = spark.read.option(\"sep\", \" \")\\\n                .option(\"inferSchema\",\"true\")\\\n                .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n                .text(\"s3a://gamestreams/gamestream.txt\")\n\ngamestream = gamestream.withColumn(\"id\", split(gamestream.value, \" \").getItem(0))\\\n                .withColumn(\"timestamp\", split(gamestream.value,\" \").getItem(1))\\\n                .withColumn(\"teamId\", split(gamestream.value,\" \").getItem(2))\\\n                .withColumn(\"JerseyNumber\", split(gamestream.value,\" \").getItem(3))\\\n                .withColumn(\"goal_1\", split(gamestream.value,\" \").getItem(4)).drop(\"value\")\\\n                .withColumn(\"goal_1\",col(\"goal_1\").cast(\"int\"))\n\nTeamScore = gamestream.groupBy(\"teamId\").sum(\"goal_1\").withColumnRenamed(\"sum(goal_1)\", \"TeamGoals\").withColumnRenamed(\"teamId\", \"teamscore_teamid\")\nLastEventId = gamestream.orderBy(col(\"timestamp\").asc()).select(\"id\").first()[0]\nLastTimeStamp = gamestream.orderBy(col(\"timestamp\").asc()).select(\"timestamp\").first()[0]\n\ngoals = gamestream.groupby(\"teamId\",\"JerseyNumber\").sum(\"goal_1\")\\\n            .withColumnRenamed(\"sum(goal_1)\",\"Goals\").withColumnRenamed(\"teamId\", \"goals_teamId\")\\\n            .withColumnRenamed(\"JerseyNumber\", \"goals_JerseyNumber\")\nshots = gamestream.groupby(\"teamId\",\"JerseyNumber\").count()\\\n        .withColumnRenamed(\"count\",\"ShotsTaken\").withColumnRenamed(\"teamId\", \"shots_teamId\")\\\n            .withColumnRenamed(\"JerseyNumber\", \"shots_JerseyNumber\")\nQ4 = goals.join(shots, (goals.goals_teamId == shots.shots_teamId)&(goals.goals_JerseyNumber == shots.shots_JerseyNumber))\\\n    .select(col(\"goals_teamId\").alias(\"teamId\"),col(\"goals_JerseyNumber\").alias(\"JerseyNumber\"),\"Goals\",\"ShotsTaken\")\n\nQ4 = Q4.join(TeamScore, Q4.teamId == TeamScore.teamscore_teamid)\\\n    .withColumn(\"eventId\",lit(LastEventId)).withColumn(\"timestamp\",lit(LastTimeStamp))\\\n    .select(\"eventId\",\"timestamp\",\"teamId\",\"JerseyNumber\",\"ShotsTaken\",\"Goals\",\"TeamGoals\")\\\n    .where(col(\"teamId\") != 0)\n\nplayers = players.withColumnRenamed(\"teamid\", \"player_team_id\").withColumnRenamed(\"goals\", \"player_goal\")\nteams = teams.withColumnRenamed(\"name\", \"teamname\")\n\nteamgoals = gamestream.groupBy(\"teamId\").sum(\"goal_1\").withColumnRenamed(\"sum(goal_1)\", \"TeamScore\").withColumnRenamed(\"teamId\", \"teamgoals_teamId\")\n\nHomeScore = teamgoals.select('TeamScore').where(col(\"teamgoals_teamId\") == 101).first()[0]\nAwayScore = teamgoals.select('TeamScore').where(col(\"teamgoals_teamId\") == 205).first()[0]\n\n## Q5\nQ5 = Q4.join(players, (Q4.JerseyNumber == players.number) & (Q4.teamId == players.player_team_id)).join(teams, Q4.teamId == teams.id)\\\n    .join(teamgoals, Q4.teamId == teamgoals.teamgoals_teamId)\\\n    .withColumn(\"HomeScore\", lit(HomeScore)).withColumn(\"AwayScore\",lit(AwayScore))\\\n    .select(\"eventId\",\"timestamp\",\"teamId\", \"JerseyNumber\", \"ShotsTaken\", \"Goals\", col(\"name\").alias(\"PlayerName\"), \n            \"teamname\", \"wins\",\"losses\", \"conference\",\"TeamScore\",\"HomeScore\",\"AwayScore\")\\\n    .withColumn(\"HomeScore\", col(\"HomeScore\").cast(\"int\"))\\\n    .withColumn(\"AwayScore\", col(\"AwayScore\").cast(\"int\"))\\\n    .withColumn(\"pct\", expr(\"Goals/ShotsTaken\"))\n\n#Status\nQ5 =Q5.withColumn(\"status\", F.when((col(\"teamId\") == 101) & (col(\"HomeScore\") > col(\"AwayScore\")), lit(\"winning\"))\\\n                 .when((col(\"teamId\") == 205) & (col(\"HomeScore\") > col(\"AwayScore\")),lit(\"losing\"))\\\n              .when((col(\"teamId\") == 101) & (col(\"HomeScore\") < col(\"AwayScore\")),lit(\"losing\"))\\\n              .when((col(\"teamId\") == 205) & (col(\"HomeScore\") < col(\"AwayScore\")),lit(\"winning\")).otherwise(lit(\"Tied\")))\n                \n            \n\n## Q6\nhome = Q5.where(col(\"teamId\") == 101).withColumnRenamed(\"timestamp\", \"home_timestamp\")\naway = Q5.where(col(\"teamId\") == 205).withColumnRenamed(\"timestamp\", \"away_timestamp\")\n\n## Q6 struct\nplayers_struct = struct(\"PlayerName\",\"JerseyNumber\",\"Goals\",\"ShotsTaken\",\"pct\")\nhome_df = home.groupBy(\"eventId\",\"home_timestamp\", \"teamname\",\"teamId\",\"TeamScore\",\"conference\",\"wins\",\"losses\",\"status\")\\\n            .agg(collect_list(players_struct).alias(\"home_players\"))\\\n.withColumn(\"home\", struct(col(\"teamId\"),col('teamname'),col('TeamScore').alias(\"Score\"), col(\"status\"),\n                           col(\"conference\"),col(\"wins\"),col(\"losses\"),\"home_players\"))\\\n.select('eventId','home_timestamp','home').withColumnRenamed(\"eventId\",\"home_eventId\")\n\n\naway_df = away.groupBy(\"eventId\",\"away_timestamp\", \"teamname\",\"teamId\",\"TeamScore\",\"conference\",\"wins\",\"losses\",\"status\")\\\n            .agg(collect_list(players_struct).alias(\"away_players\"))\\\n.withColumn(\"away\", struct(col(\"teamId\"),col('teamname'),col('TeamScore').alias(\"Score\"),col(\"status\"),\n                           col(\"conference\"),col(\"wins\"),col(\"losses\"),\"away_players\"))\\\n.select('eventId','away_timestamp','away').withColumnRenamed(\"eventId\",\"away_eventId\")\n\nQ6 = home_df.join(away_df, (home_df.home_timestamp == away_df.away_timestamp), \"outer\")\\\n    .select(col(\"home_eventId\").alias(\"eventId\"), col(\"home_timestamp\").alias(\"timestamp\"), 'home', 'away')\n\n# ## Q7\nQ6.withColumn(\"_id\",Q6.eventId).write.format(\"mongo\").mode(\"append\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscores\").save()",
      "metadata": {},
      "execution_count": 26,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        }
      ],
      "id": "fe5c29b8-f4ea-4ec1-9c86-8657eb440970"
    },
    {
      "cell_type": "code",
      "source": "## 11th \nif gamestream.count() == 71:\n    score = gamestream.groupBy(\"teamId\").sum(\"goal_1\").where(col(\"teamId\") != 0)\n    home_score = score.where(col(\"teamId\") == \"101\").first()[1]\n    away_score = score.where(col(\"teamId\") == \"205\").first()[1]\n    if home_score > away_score:\n        teams = teams.withColumn(\"wins\", F.when(teams.id == \"101\",expr(\"wins + 1\")).when(teams.id == \"205\", expr(\"wins + 0\")))\n        teams = teams.withColumn(\"losses\", F.when(teams.id == \"101\",expr(\"losses + 0\")).when(teams.id == \"205\", expr(\"losses + 1\")))\n    ## 12th. Creating a teams2 table in mssql\n        teams.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .mode(\"overwrite\") \\\n        .option(\"url\", mssql_url) \\\n        .option(\"dbtable\", \"teams2\") \\\n        .option(\"user\", mssql_user) \\\n        .option(\"password\", mssql_pw) \\\n        .save()\n    if home_score < away_score:\n        teams = teams.withColumn(\"wins\", F.when(teams.id == \"101\",expr(\"wins + 0\")).when(teams.id == \"205\", expr(\"wins + 1\")))\n        teams = teams.withColumn(\"losses\", F.when(teams.id == \"101\",expr(\"losses + 1\")).when(teams.id == \"205\", expr(\"losses + 0\")))\n        \n    ## 12th. Creating a teams2 table in mssql\n        teams.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .mode(\"overwrite\") \\\n        .option(\"url\", mssql_url) \\\n        .option(\"dbtable\", \"teams2\") \\\n        .option(\"user\", mssql_user) \\\n        .option(\"password\", mssql_pw) \\\n        .save()\n        \nelse:\n    teams = teams.withColumn(\"wins\", F.when(teams.id == \"101\",expr(\"wins + 0\")).when(teams.id == \"205\", expr(\"wins + 0\")))\n    teams = teams.withColumn(\"losses\", F.when(teams.id == \"101\",expr(\"losses + 0\")).when(teams.id == \"205\", expr(\"losses + 0\")))\n    \n    ## 12th. Creating a teams2 table in mssql for a draw\n    \n    teams.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .mode(\"overwrite\") \\\n        .option(\"url\", mssql_url) \\\n        .option(\"dbtable\", \"teams2\") \\\n        .option(\"user\", mssql_user) \\\n        .option(\"password\", mssql_pw) \\\n        .save()",
      "metadata": {},
      "execution_count": 18,
      "outputs": [],
      "id": "cbb8ccf0-2fc1-46b9-ad0d-71ff82b23f6e"
    },
    {
      "cell_type": "code",
      "source": "## 13 & 14\n\nif gamestream.count() == 71:\n    player_details = gamestream.groupby(\"teamId\",\"JerseyNumber\").pivot(\"goal_1\").count()\\\n    .withColumnRenamed(\"0\", \"MissedShots\")\\\n    .withColumnRenamed(\"1\", \"Goals\")\\\n    .na.fill(value = 0, subset = [\"Goals\",\"MissedShots\"])\\\n    .withColumn(\"ShotsTaken\", expr(\"MissedShots + Goals\")).where(col(\"teamId\") != 0)\n    \n    players = player_details.join(players, (player_details.teamId == players.player_team_id) & (player_details.JerseyNumber == players.number))\\\n                .withColumn(\"shots\", expr(\"shots + ShotsTaken\")).withColumn(\"player_goal\",expr(\"player_goal + Goals\"))\\\n                .select(\"id\",\"name\",\"number\",\"shots\",\"player_goal\",\"player_team_id\")\n    \n    \n    players.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        .mode(\"overwrite\") \\\n        .option(\"url\", mssql_url) \\\n        .option(\"dbtable\", \"players2\") \\\n        .option(\"user\", mssql_user) \\\n        .option(\"password\", mssql_pw) \\\n        .save()\n                                                                            \n    ",
      "metadata": {
        "tags": []
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        }
      ],
      "id": "58f6abbe-077f-4b18-8a43-0bae732330d8"
    }
  ]
}